{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务2：梯度计算和梯度下降过程任务要点：Pytorch梯度计算、随机梯度下降**\n",
    "- 步骤1：学习自动求梯度原理，https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "- 步骤2：学习随机梯度下降原理，https://www.cnblogs.com/BYRans/p/4700202.html\n",
    "- 步骤3：使用numpy创建一个y=10*x+4+noise(0,1)的数据，其中x是0到100的范围，以0.01进行等差数列,使用pytroch定义w和b，并使用随机梯度下降，完成回归拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生数据\n",
    "x_data = np.arange(0,1,0.01)\n",
    "y_data = 10*x_data + 4 + np.random.normal(0,1,100)\n",
    "x = torch.from_numpy(x_data)\n",
    "y = torch.from_numpy(y_data)\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= tensor([1.3141], requires_grad=True)\n",
      "b= tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# print(\"x=\",x)\n",
    "# print(\"y=\",y)\n",
    "print(\"w=\",w)\n",
    "print(\"b=\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= tensor(72.9849, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 预测值\n",
    "y_head = w * x + b\n",
    "\n",
    "# loss 计算\n",
    "loss = torch.mean((y_head - y)**2)\n",
    "\n",
    "print(\"loss=\",loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.4905])\n",
      "tensor([-16.1761])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    }
   ],
   "source": [
    "# 计算梯度\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新 w,b\n",
    "lr = 1e-2\n",
    "w.data = w.data - lr * w.grad.data\n",
    "b.data = b.data - lr * b.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= tensor([1.4090], requires_grad=True)\n",
      "b= tensor([0.1618], requires_grad=True)\n",
      "True\n",
      "tensor([-16.1761])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"w=\",w)\n",
    "print(\"b=\",b)\n",
    "print(w.requires_grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 69.51184937254087\n",
      "epoch: 10, loss: 42.998359967626136\n",
      "epoch: 20, loss: 27.075494991025135\n",
      "epoch: 30, loss: 17.503853102992306\n",
      "epoch: 40, loss: 11.741307764348448\n",
      "epoch: 50, loss: 8.26345179965591\n",
      "epoch: 60, loss: 6.156181413551235\n",
      "epoch: 70, loss: 4.871341713538274\n",
      "epoch: 80, loss: 4.080221669754194\n",
      "epoch: 90, loss: 3.585693563710976\n",
      "epoch: 100, loss: 3.2695428442976318\n",
      "epoch: 110, loss: 3.060865691893939\n",
      "epoch: 120, loss: 2.9171317580632\n",
      "epoch: 130, loss: 2.8128256275416885\n",
      "epoch: 140, loss: 2.7326359705845555\n",
      "epoch: 150, loss: 2.6673658526838127\n",
      "epoch: 160, loss: 2.61149140012904\n",
      "epoch: 170, loss: 2.561686684079121\n",
      "epoch: 180, loss: 2.515946972881058\n",
      "epoch: 190, loss: 2.473058947416128\n",
      "epoch: 200, loss: 2.432283769030256\n",
      "epoch: 210, loss: 2.39316813612567\n",
      "epoch: 220, loss: 2.3554301275162914\n",
      "epoch: 230, loss: 2.3188901522625853\n",
      "epoch: 240, loss: 2.2834315278326227\n",
      "epoch: 250, loss: 2.248974935207971\n",
      "epoch: 260, loss: 2.2154627069532045\n",
      "epoch: 270, loss: 2.1828526193437257\n",
      "epoch: 280, loss: 2.1511095627083514\n",
      "epoch: 290, loss: 2.1202036929106196\n",
      "epoch: 300, loss: 2.0901100945101034\n",
      "epoch: 310, loss: 2.060804735734875\n",
      "epoch: 320, loss: 2.0322652731402853\n",
      "epoch: 330, loss: 2.004471553717768\n",
      "epoch: 340, loss: 1.9774031211518814\n",
      "epoch: 350, loss: 1.9510408724539317\n",
      "epoch: 360, loss: 1.9253667549242868\n",
      "epoch: 370, loss: 1.9003617501426715\n",
      "epoch: 380, loss: 1.8760089979469092\n",
      "epoch: 390, loss: 1.8522914699698865\n",
      "epoch: 400, loss: 1.8291925412111516\n",
      "epoch: 410, loss: 1.806695663143517\n",
      "epoch: 420, loss: 1.7847856409536313\n",
      "epoch: 430, loss: 1.763446739699623\n",
      "epoch: 440, loss: 1.7426643625611495\n",
      "epoch: 450, loss: 1.7224233754782687\n",
      "epoch: 460, loss: 1.7027104199845042\n",
      "epoch: 470, loss: 1.6835114831186826\n",
      "epoch: 480, loss: 1.6648132528285509\n",
      "epoch: 490, loss: 1.646602530202506\n",
      "epoch: 500, loss: 1.6288666924520334\n",
      "epoch: 510, loss: 1.61159340329746\n",
      "epoch: 520, loss: 1.59477042574865\n",
      "epoch: 530, loss: 1.578386077484867\n",
      "epoch: 540, loss: 1.562428843436788\n",
      "epoch: 550, loss: 1.5468877367776337\n",
      "epoch: 560, loss: 1.5317520944032401\n",
      "epoch: 570, loss: 1.5170110757972264\n",
      "epoch: 580, loss: 1.5026542646011898\n",
      "epoch: 590, loss: 1.488671891962827\n",
      "epoch: 600, loss: 1.4750536967214836\n",
      "epoch: 610, loss: 1.461790967172023\n",
      "epoch: 620, loss: 1.4488739559897816\n",
      "epoch: 630, loss: 1.4362939129466994\n",
      "epoch: 640, loss: 1.424041754643926\n",
      "epoch: 650, loss: 1.412109124643698\n",
      "epoch: 660, loss: 1.400487785157251\n",
      "epoch: 670, loss: 1.3891693942868415\n",
      "epoch: 680, loss: 1.378145955038118\n",
      "epoch: 690, loss: 1.3674093096230455\n",
      "epoch: 700, loss: 1.356952917459763\n",
      "epoch: 710, loss: 1.3467697086013282\n",
      "epoch: 720, loss: 1.336852225030936\n",
      "epoch: 730, loss: 1.32719320489588\n",
      "epoch: 740, loss: 1.3177859923593678\n",
      "epoch: 750, loss: 1.3086238953988043\n",
      "epoch: 760, loss: 1.2997007764362731\n",
      "epoch: 770, loss: 1.2910102387557503\n",
      "epoch: 780, loss: 1.282546652931914\n",
      "epoch: 790, loss: 1.2743033043301795\n",
      "epoch: 800, loss: 1.2662750781898782\n",
      "epoch: 810, loss: 1.2584560224564865\n",
      "epoch: 820, loss: 1.250841085430688\n",
      "epoch: 830, loss: 1.2434245147896064\n",
      "epoch: 840, loss: 1.2362015560910935\n",
      "epoch: 850, loss: 1.229166487222753\n",
      "epoch: 860, loss: 1.222315386317253\n",
      "epoch: 870, loss: 1.2156428221167412\n",
      "epoch: 880, loss: 1.2091440460787934\n",
      "epoch: 890, loss: 1.2028148854044036\n",
      "epoch: 900, loss: 1.1966507391173211\n",
      "epoch: 910, loss: 1.1906479913170673\n",
      "epoch: 920, loss: 1.1848010651282785\n",
      "epoch: 930, loss: 1.1791066834267898\n",
      "epoch: 940, loss: 1.1735605026710298\n",
      "epoch: 950, loss: 1.1681589129259342\n",
      "epoch: 960, loss: 1.1628982499058944\n",
      "epoch: 970, loss: 1.1577749301042513\n",
      "epoch: 980, loss: 1.1527851521768477\n",
      "epoch: 990, loss: 1.1479254905381828\n",
      "epoch: 1000, loss: 1.1431925590600485\n",
      "epoch: 1010, loss: 1.138582906139208\n",
      "epoch: 1020, loss: 1.1340935651579531\n",
      "epoch: 1030, loss: 1.1297213729146958\n",
      "epoch: 1040, loss: 1.1254631478729886\n",
      "epoch: 1050, loss: 1.121315739930042\n",
      "epoch: 1060, loss: 1.117276714643252\n",
      "epoch: 1070, loss: 1.1133429280634923\n",
      "epoch: 1080, loss: 1.1095118523676368\n",
      "epoch: 1090, loss: 1.105780526313068\n",
      "epoch: 1100, loss: 1.1021464165598882\n",
      "epoch: 1110, loss: 1.098607138699248\n",
      "epoch: 1120, loss: 1.0951601526415318\n",
      "epoch: 1130, loss: 1.0918030597021982\n",
      "epoch: 1140, loss: 1.0885335158960976\n",
      "epoch: 1150, loss: 1.0853492704843588\n",
      "epoch: 1160, loss: 1.0822478978504595\n",
      "epoch: 1170, loss: 1.0792275472501172\n",
      "epoch: 1180, loss: 1.0762858565156737\n",
      "epoch: 1190, loss: 1.0734207416265196\n",
      "epoch: 1200, loss: 1.0706306607135296\n",
      "epoch: 1210, loss: 1.0679129171095614\n",
      "epoch: 1220, loss: 1.0652659507379376\n",
      "epoch: 1230, loss: 1.0626882989019648\n",
      "epoch: 1240, loss: 1.0601779556227764\n",
      "epoch: 1250, loss: 1.0577331321820855\n",
      "epoch: 1260, loss: 1.0553518500079593\n",
      "epoch: 1270, loss: 1.0530327349698596\n",
      "epoch: 1280, loss: 1.0507741436956106\n",
      "epoch: 1290, loss: 1.0485745114307243\n",
      "epoch: 1300, loss: 1.0464321790100488\n",
      "epoch: 1310, loss: 1.0443455929160228\n",
      "epoch: 1320, loss: 1.0423134123358297\n",
      "epoch: 1330, loss: 1.040334213194721\n",
      "epoch: 1340, loss: 1.038406713400235\n",
      "epoch: 1350, loss: 1.0365294887411276\n",
      "epoch: 1360, loss: 1.0347011498511807\n",
      "epoch: 1370, loss: 1.0329205056457391\n",
      "epoch: 1380, loss: 1.0311862862710552\n",
      "epoch: 1390, loss: 1.02949727401972\n",
      "epoch: 1400, loss: 1.0278523077259212\n",
      "epoch: 1410, loss: 1.0262503020794984\n",
      "epoch: 1420, loss: 1.0246900964966692\n",
      "epoch: 1430, loss: 1.0231704761410907\n",
      "epoch: 1440, loss: 1.021690488267389\n",
      "epoch: 1450, loss: 1.0202492254739335\n",
      "epoch: 1460, loss: 1.0188453948733824\n",
      "epoch: 1470, loss: 1.0174782165667053\n",
      "epoch: 1480, loss: 1.0161466470723572\n",
      "epoch: 1490, loss: 1.0148498404391941\n",
      "epoch: 1500, loss: 1.0135868736144296\n",
      "epoch: 1510, loss: 1.0123566841392821\n",
      "epoch: 1520, loss: 1.0111587644883975\n",
      "epoch: 1530, loss: 1.0099920901394916\n",
      "epoch: 1540, loss: 1.0088557073260216\n",
      "epoch: 1550, loss: 1.0077489989938748\n",
      "epoch: 1560, loss: 1.0066710274394126\n",
      "epoch: 1570, loss: 1.005621315104948\n",
      "epoch: 1580, loss: 1.004598857257596\n",
      "epoch: 1590, loss: 1.0036031589524856\n",
      "epoch: 1600, loss: 1.0026334611678407\n",
      "epoch: 1610, loss: 1.001688985826421\n",
      "epoch: 1620, loss: 1.0007690936044957\n",
      "epoch: 1630, loss: 0.9998733434373094\n",
      "epoch: 1640, loss: 0.99900076263944\n",
      "epoch: 1650, loss: 0.998151032658489\n",
      "epoch: 1660, loss: 0.9973234531962819\n",
      "epoch: 1670, loss: 0.9965173982784609\n",
      "epoch: 1680, loss: 0.995732361312336\n",
      "epoch: 1690, loss: 0.9949677337574856\n",
      "epoch: 1700, loss: 0.9942233125448198\n",
      "epoch: 1710, loss: 0.9934979442524725\n",
      "epoch: 1720, loss: 0.9927918658574022\n",
      "epoch: 1730, loss: 0.9921041258274198\n",
      "epoch: 1740, loss: 0.9914342598148101\n",
      "epoch: 1750, loss: 0.9907818902688645\n",
      "epoch: 1760, loss: 0.9901466545211264\n",
      "epoch: 1770, loss: 0.989527602929062\n",
      "epoch: 1780, loss: 0.988924909715474\n",
      "epoch: 1790, loss: 0.988337821438395\n",
      "epoch: 1800, loss: 0.987766109334996\n",
      "epoch: 1810, loss: 0.9872093482580396\n",
      "epoch: 1820, loss: 0.9866670463114944\n",
      "epoch: 1830, loss: 0.9861388592080553\n",
      "epoch: 1840, loss: 0.9856244098324578\n",
      "epoch: 1850, loss: 0.985123428447252\n",
      "epoch: 1860, loss: 0.9846354932560931\n",
      "epoch: 1870, loss: 0.9841602900984324\n",
      "epoch: 1880, loss: 0.9836974810713585\n",
      "epoch: 1890, loss: 0.9832467659582399\n",
      "epoch: 1900, loss: 0.9828078140069658\n",
      "epoch: 1910, loss: 0.9823802767718\n",
      "epoch: 1920, loss: 0.9819639158594555\n",
      "epoch: 1930, loss: 0.9815583829518567\n",
      "epoch: 1940, loss: 0.9811634059096679\n",
      "epoch: 1950, loss: 0.9807788085783369\n",
      "epoch: 1960, loss: 0.9804041598160085\n",
      "epoch: 1970, loss: 0.9800393092100083\n",
      "epoch: 1980, loss: 0.9796839569373138\n",
      "epoch: 1990, loss: 0.9793378837134799\n",
      "epoch: 2000, loss: 0.9790008223323855\n",
      "epoch: 2010, loss: 0.9786725638744491\n",
      "epoch: 2020, loss: 0.9783528686580268\n",
      "epoch: 2030, loss: 0.9780415213484894\n",
      "epoch: 2040, loss: 0.9777383051997862\n",
      "epoch: 2050, loss: 0.9774429508703045\n",
      "epoch: 2060, loss: 0.9771553374688328\n",
      "epoch: 2070, loss: 0.9768751672274876\n",
      "epoch: 2080, loss: 0.9766023268685481\n",
      "epoch: 2090, loss: 0.9763365853515868\n",
      "epoch: 2100, loss: 0.9760777750418579\n",
      "epoch: 2110, loss: 0.9758257230167144\n",
      "epoch: 2120, loss: 0.9755802850627248\n",
      "epoch: 2130, loss: 0.9753412374864732\n",
      "epoch: 2140, loss: 0.9751083632220285\n",
      "epoch: 2150, loss: 0.9748816110684149\n",
      "epoch: 2160, loss: 0.9746607363415996\n",
      "epoch: 2170, loss: 0.9744456525644458\n",
      "epoch: 2180, loss: 0.974236163445033\n",
      "epoch: 2190, loss: 0.9740321440989607\n",
      "epoch: 2200, loss: 0.9738334444669785\n",
      "epoch: 2210, loss: 0.9736399278988828\n",
      "epoch: 2220, loss: 0.9734513956026515\n",
      "epoch: 2230, loss: 0.9732678582370954\n",
      "epoch: 2240, loss: 0.9730890343262169\n",
      "epoch: 2250, loss: 0.9729149583010225\n",
      "epoch: 2260, loss: 0.9727454316504859\n",
      "epoch: 2270, loss: 0.9725803215991252\n",
      "epoch: 2280, loss: 0.9724195166113612\n",
      "epoch: 2290, loss: 0.9722628062535157\n",
      "epoch: 2300, loss: 0.9721102339489893\n",
      "epoch: 2310, loss: 0.9719616204468571\n",
      "epoch: 2320, loss: 0.971816920039021\n",
      "epoch: 2330, loss: 0.9716759606981994\n",
      "epoch: 2340, loss: 0.9715386839876393\n",
      "epoch: 2350, loss: 0.9714049785705169\n",
      "epoch: 2360, loss: 0.9712747783095419\n",
      "epoch: 2370, loss: 0.9711479502894881\n",
      "epoch: 2380, loss: 0.9710244462965179\n",
      "epoch: 2390, loss: 0.9709041715414805\n",
      "epoch: 2400, loss: 0.9707870298469682\n",
      "epoch: 2410, loss: 0.9706729201002983\n",
      "epoch: 2420, loss: 0.9705617830622731\n",
      "epoch: 2430, loss: 0.9704535914456599\n",
      "epoch: 2440, loss: 0.9703481868356622\n",
      "epoch: 2450, loss: 0.9702455185946096\n",
      "epoch: 2460, loss: 0.9701455560488431\n",
      "epoch: 2470, loss: 0.9700481862316529\n",
      "epoch: 2480, loss: 0.9699533583778099\n",
      "epoch: 2490, loss: 0.9698610156510149\n",
      "epoch: 2500, loss: 0.9697710553668765\n",
      "epoch: 2510, loss: 0.9696834558958983\n",
      "epoch: 2520, loss: 0.9695981427878421\n",
      "epoch: 2530, loss: 0.9695150372830227\n",
      "epoch: 2540, loss: 0.9694341200646315\n",
      "epoch: 2550, loss: 0.9693553037640187\n",
      "epoch: 2560, loss: 0.9692785349154329\n",
      "epoch: 2570, loss: 0.9692037535180634\n",
      "epoch: 2580, loss: 0.9691309147481075\n",
      "epoch: 2590, loss: 0.9690599998391457\n",
      "epoch: 2600, loss: 0.9689909513602675\n",
      "epoch: 2610, loss: 0.9689236773232417\n",
      "epoch: 2620, loss: 0.9688581675662431\n",
      "epoch: 2630, loss: 0.968794366024431\n",
      "epoch: 2640, loss: 0.9687322363075188\n",
      "epoch: 2650, loss: 0.9686717193043168\n",
      "epoch: 2660, loss: 0.968612780721639\n",
      "epoch: 2670, loss: 0.9685553868241299\n",
      "epoch: 2680, loss: 0.9684994898859426\n",
      "epoch: 2690, loss: 0.9684450357313757\n",
      "epoch: 2700, loss: 0.9683920104140554\n",
      "epoch: 2710, loss: 0.9683403691340442\n",
      "epoch: 2720, loss: 0.9682900633652131\n",
      "epoch: 2730, loss: 0.9682410831036051\n",
      "epoch: 2740, loss: 0.9681933605858362\n",
      "epoch: 2750, loss: 0.9681468918642906\n",
      "epoch: 2760, loss: 0.9681016295541647\n",
      "epoch: 2770, loss: 0.9680575675790215\n",
      "epoch: 2780, loss: 0.9680146472402975\n",
      "epoch: 2790, loss: 0.9679728439417167\n",
      "epoch: 2800, loss: 0.9679321290245607\n",
      "epoch: 2810, loss: 0.9678924855126334\n",
      "epoch: 2820, loss: 0.9678538734996199\n",
      "epoch: 2830, loss: 0.9678162521946212\n",
      "epoch: 2840, loss: 0.9677796184290918\n",
      "epoch: 2850, loss: 0.9677439526343187\n",
      "epoch: 2860, loss: 0.9677091960411159\n",
      "epoch: 2870, loss: 0.9676753526743724\n",
      "epoch: 2880, loss: 0.9676424024079421\n",
      "epoch: 2890, loss: 0.9676103104733275\n",
      "epoch: 2900, loss: 0.9675790432347884\n",
      "epoch: 2910, loss: 0.9675485992986403\n",
      "epoch: 2920, loss: 0.9675189459814668\n",
      "epoch: 2930, loss: 0.9674900658992656\n",
      "epoch: 2940, loss: 0.9674619297964479\n",
      "epoch: 2950, loss: 0.9674345490717513\n",
      "epoch: 2960, loss: 0.9674078810918202\n",
      "epoch: 2970, loss: 0.9673818830838795\n",
      "epoch: 2980, loss: 0.9673565672212134\n",
      "epoch: 2990, loss: 0.9673319217326565\n"
     ]
    }
   ],
   "source": [
    "# 进行30次更新：\n",
    "for i in range(3000):\n",
    "    y_head = w*x + b\n",
    "#     print(\"w=\",w)\n",
    "#     print(\"b=\",b)\n",
    "    loss = torch.mean((y_head - y)**2)\n",
    "\n",
    "\n",
    "    # 梯度归零, 因为前面已经求过一次了 ，所以可以直接清零，否则的话需要经过下面判断，\n",
    "    #     if i != 0:\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "        # print(\"w.grad\",w.grad)\n",
    "        # print(\"b.grad\",b.grad)\n",
    "    #     print(\"x=\",x)\n",
    "    #     print(\"y=\",y)\n",
    "    #     print(\"y_=\",y_head)\n",
    "\n",
    "    # 自动求导\n",
    "    loss.backward()\n",
    "#     print(loss)\n",
    "    # print(loss.grad_fn)\n",
    "#     print(\"w.grad\",w.grad)\n",
    "#     print(\"b.grad\",b.grad)\n",
    "    # 更新参数\n",
    "    lr = 1e-2\n",
    "    w.data = w.data - lr * w.grad.data\n",
    "    b.data = b.data - lr * b.grad.data\n",
    "    if i % 10 == 0:\n",
    "        print('epoch: {}, loss: {}'.format(i, loss.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  tensor([10.1122], requires_grad=True)\n",
      "b =  tensor([3.7366], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"w = \", w)\n",
    "print(\"b = \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:因为没有用.data导致的“非计算图叶子节点”错误**\n",
    "- 回答：https://blog.csdn.net/zzhhjjjj/article/details/112912971\n",
    "- 出错原因：\n",
    "    - 将\n",
    "        ``` python\n",
    "        w.data = w.data - lr * w.grad.data\n",
    "        b.data = b.data - lr * b.grad.data\n",
    "        ```\n",
    "      写为：\n",
    "        ``` python\n",
    "        w = w - lr * w.grad\n",
    "        b = b - lr * b.grad\n",
    "        ```\n",
    "**Q2:因为没有清零梯度导致的错误**\n",
    "- 回答： https://blog.csdn.net/m0_37637704/article/details/101019438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
